**Deep Learning**
it refers to taining neural networks.

problem with sigmoid activation function-*One of the major disadvantages of using sigmoid is the problem of vanishing gradient*

**computation graph**

https://www.geeksforgeeks.org/computational-graphs-in-deep-learning/

Computations of the neural network are organized in terms of a forward pass or **forward propagation** step in which we compute the **output** of the neural network, followed by a backward pass or  **backward propagation** step, which we use to compute **gradients/derivatives**. Computation graphs explain why it is organized this way. 


**logistic regression**

https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc

**vectorisation**

Vectorization is the process of converting an algorithm from operating on a single value at a time to operating on a set of values (vector) at one time. 

Vectorization allows the elimination of the for-loops in python code. It is especially important in Deep learning as we are dealing with large numbers of datasets. So, it allows the code to run quickly and helps train the algorithms faster


